# Lookahead Decoding

Unlike the single-token generation in autoregressive decoding, lookahead decoding generates multiple tokens simultaneously, adequately utilizing the parallel processing capabilities of the GPU, leveraging computation (FLOPs) for latency reduction. Moreover, lookahead decoding doesn’t require a separate draft model that’s needed for draft target speculative decoding.

Each decoding step is divided into two parallel branches, the lookahead branch and the verification branch. Using the [**Jacobi iteration method**](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5385084), a classic nonlinear systems solver, the lookahead branch performs parallel decoding for future tokens by generating n-grams. The verification branch selects and verifies the promising n-gram candidates generated by the lookahead branch.

[Learn more here](https://developer.nvidia.com/blog/optimizing-qwen2-5-coder-throughput-with-nvidia-tensorrt-llm-lookahead-decoding/)

## Using Lookahead with Baseten’s TensorRT-LLM Engine Builder

Adding lookahead decoding to your TRT engine builder is as simple as adding a `speculator` section to your Engine Builder configuration:

```
...
trt_llm:
  build:
    ...
    speculator:
      lookahead_windows_size: 3
      lookahead_ngram_size: 7
      lookahead_verification_set_size: 2
      speculative_decoding_mode: LOOKAHEAD_DECODING
    ...
  runtime:
    ...
```

The lookahead algorithm is configured using three key parameters: window size (W), n-gram size (N), and verification set size (G).

- **Window size (W):** Represents the lookahead window size, which determines how many future tokens the algorithm attempts to predict in each step. Larger window size enables the model to look further, helping generate more tokens in a single pass. This effectively improves throughput performance while utilizing GPU computation FLOPs efficiently.
- **N-gram size (N):** Represents the size of the n-grams used in the lookahead process. For example, a 5-gram is a contiguous sequence of 5 future tokens. Together with the window size, it creates a fixed-sized, 2D window for the lookahead branch to generate n-grams from the Jacobi iteration trajectory.
- **Verification set size (G):** Represents the maximum number of speculations or candidate n-grams that the algorithm considers in each step for verification. It balances the trade-off between computation efficiency and exploring more possibilities.

## Optimizing Your Model

Since the most optimal combination of these settings is use case dependent, determining the right values for your use case is key to maximizing performance of your model. Luckily TRT allows us to modify these values on the fly, by leveraging this capability we are able to automatically run performance sweeps across 100s of combinations to determine the optimal settings for your specific model and use case.

### Setup

- **Sample Requests** - A collection of requests saved in a json format. The closer these requests are to your production traffic the better the results will be.

- **Testing Model** - When running the performance optimizations you will need to deploy a model to Baseten with lookahead decoding enabled. **Important!!** _This model must be configured to have the maximum lookahead settings that you will test for, if you try and test any values above those set in the model, it will crash!_

- **Baseten API Key** - Create your API key [here](https://app.baseten.co/settings/api_keys) so the performance tests are able to call the testing model's API.

**Example testing model configuration:**

```
 speculator:
      lookahead_windows_size: 3
      lookahead_ngram_size: 64
      lookahead_verification_set_size: 5
      speculative_decoding_mode: LOOKAHEAD_DECODING
```

### Running the Performance Tests

Simply run the lookahead performance script with your Baseten model id, api key and the lookahead settings that the test model was deployed with. Ex:

```
python lookahead.py --model {baseten model id} --api_key {api key} --window_size 3 --ngram_size 64 --verification_size 5
```

**--model** - (Required) The model URL to send requests to

**--api_key** - (Required) The baseten API key for the model

**--requests_dir** - (Optional) The directory containing the requests to send to the model. Each request should be a separate JSON file. [default=./requests]

**--max_requests** - (Optional) The maximum number of requests to use from the request_dir. This is useful for running a subset of the requests directory.

**--request_concurrency** - (Optional) The maximum number of parallel requests to send to the model. [default=10]

**--window_size** - (Required) The maximum window size for the lookahead decoding. This should match the configuration the engine was built with.

**--ngram_size** - (Required) The maximum ngram size for the lookahead decoding. This should match the configuration the engine was built with.

**--verification_size** - (Required) The maximum ngram size for the lookahead decoding. This should match the configuration the engine was built with.

### Interpreting Results

The performance tests will print out the lookahead decoding settings that resulted in the lowest E2E latency (P50, P90, P99) as well as graphs that show the latency vs lookahead configuration. We suggest finding the right balance between P50 & P90 latency, for some use cases we have seen the optimal P50 result in very bad P90s and selected a configuration that kept the overall system performance in mind.

### Deploying a Production Model

Once you have selected the lookahead configuration that best fits your use case, we suggest deploying a new model to your account with the engine builder with your optimal configuration settings. In the future you can rerun these performance evaluations with new requests to ensure that you are still running the most optimal configuration for production.
